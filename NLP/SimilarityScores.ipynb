{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimilarityAssignment .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EflZaPOjtj3b",
        "outputId": "5d5db604-1e62-4547-98ac-47c421212990"
      },
      "source": [
        "#Step 1 - Corpus Preprocessing 20pts\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import *\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import brown\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import math \n",
        "\n",
        "\n",
        "\n",
        "#Create a corpus with at least three documents\n",
        "def listToString(s):  \n",
        "    stringFromlist = \"\"  \n",
        "    for ele in s:  \n",
        "        stringFromlist += ' ' + ele   \n",
        "    return stringFromlist  \n",
        "        \n",
        "\n",
        "#Create a corpus with at least three documents - 5pts\n",
        "#created string from words then concatenated for corpus to then tokenize\n",
        "#As a list\n",
        "brown.categories()\n",
        "wordsOne = brown.words(categories='adventure')\n",
        "wordsTwo = brown.words(categories='mystery')\n",
        "wordsThree = brown.words(categories='religion')\n",
        "docList = [wordsOne,wordsTwo,wordsThree]\n",
        "\n",
        "#Corpus of three documents \n",
        "corpusOfThree = stringOfOne + stringOfTwo + stringOfThree\n",
        "stringOfOne = listToString(wordsOne)\n",
        "stringOfTwo = listToString(wordsTwo)\n",
        "stringOfThree = listToString(wordsThree)\n",
        "\n",
        "#punctuation, stemming or lemmatization) - 15pt\n",
        "def normalizeCorpus(corpus):\n",
        "  #Pre-process corpus (tokenization, stopwords, punctuation, stemming or lemmatization) - 15pts\n",
        "#Tokenize \n",
        "  from nltk.tokenize import word_tokenize \n",
        "  nltk.download('punkt')\n",
        "  tokenized_corpus = word_tokenize(corpus)\n",
        "\n",
        "  #Stopwords\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  filtered_words = [w for w in tokenized_corpus if not w in stopwords]\n",
        "\n",
        "  #punctuation\n",
        "  removedpunctuation_words = [word.lower() for word in filtered_words if word.isalpha()]\n",
        "\n",
        "  #stemming\n",
        "  ps = PorterStemmer()\n",
        "  stemmed_words = [ps.stem(word) for word in removedpunctuation_words]\n",
        "\n",
        "  #lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_words = [lemmatizer.lemmatize(w) for w in stemmed_words ]\n",
        "  normalized_corpus = lemmatized_words\n",
        "  return normalized_corpus \n",
        "  #Corpus has been normalized\n",
        "\n",
        "\n",
        "#Pre-process corpus (tokenization, stopwords, punctuation, stemming or lemmatization) - 15pts\n",
        "#Tokenize \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "tokenized_corpus = word_tokenize(corpusOfThree)\n",
        "\n",
        "#Stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "filtered_words = [w for w in tokenized_corpus if not w in stopwords]\n",
        "\n",
        "#punctuation\n",
        "removedpunctuation_words = [word.lower() for word in filtered_words if word.isalpha()]\n",
        "\n",
        "#stemming\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in removedpunctuation_words]\n",
        "\n",
        "#lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(w) for w in stemmed_words ]\n",
        "normalized_corpus = lemmatized_words\n",
        "#Corpus has been normalized"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy79_O-KRxQP"
      },
      "source": [
        "Adapted from Lecture and Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TIx8W9qR0Qs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKri7uNztlSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f38735b-8a25-4b47-faa9-13bd49fbf7ae"
      },
      "source": [
        "# Step 2 - Frequency 20pts\n",
        "\n",
        "import pandas as pd\n",
        "import math \n",
        "\n",
        "first_sentence = stringOfOne.split(\" \")\n",
        "second_sentence = stringOfTwo.split(\" \")\n",
        "third_sentence = stringOfThree.split(\" \")\n",
        "total= set(first_sentence).union(set(second_sentence)).union(set(third_sentence))\n",
        "wordDictA = dict.fromkeys(total, 0) \n",
        "wordDictB = dict.fromkeys(total, 0)\n",
        "wordDictC = dict.fromkeys(total, 0)\n",
        "for word in first_sentence:\n",
        "    wordDictA[word]+=1\n",
        "    \n",
        "for word in second_sentence:\n",
        "    wordDictB[word]+=1\n",
        "\n",
        "        \n",
        "for word in second_sentence:\n",
        "    wordDictC[word]+=1\n",
        "\n",
        "pd.DataFrame([wordDictA, wordDictB, wordDictC ])\n",
        "\n",
        "#Method for computing Term Frequency\n",
        "def computeTF(wordDict, doc):\n",
        "    tfDict = {}\n",
        "    corpusCount = len(doc)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count/float(corpusCount)\n",
        "    return(tfDict)\n",
        "#running our sentences through the tf function:\n",
        "tfFirst = computeTF(wordDictA, first_sentence)\n",
        "tfSecond = computeTF(wordDictB, second_sentence)\n",
        "tfThird = computeTF(wordDictC, third_sentence)\n",
        "#Converting to dataframe for visualization\n",
        "tf = pd.DataFrame([tfFirst, tfSecond, tfThird])\n",
        "print(tf)\n",
        "\n",
        "#Method for computing Inverse Document Frequency\n",
        "def computeIDF(docList):\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
        "        \n",
        "    return(idfDict)\n",
        "\n",
        "idfs = computeIDF([wordDictA, wordDictB, wordDictC])\n",
        "print(dict(list(idfs.items())[0:10]))\n",
        "print(\"\\n\")\n",
        "\n",
        "#Method for computing Term Frequency Inverse Document Frequency TF * IDF\n",
        "def computeTFIDF(tfBow, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBow.items():\n",
        "        tfidf[word] = val*idfs[word]\n",
        "    return(tfidf)\n",
        "#running our two sentences through the IDF:\n",
        "idfFirst = computeTFIDF(tfFirst, idfs)\n",
        "idfSecond = computeTFIDF(tfSecond, idfs)\n",
        "idfThird = computeTFIDF(tfThird, idfs)\n",
        "#putting it in a dataframe\n",
        "tfidf= pd.DataFrame([idfFirst, idfSecond, idfThird])\n",
        "print(tfidf)\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  I'd    rushed  ...  victory  proprietor  S-s-sahjunt\n",
            "0  0.000014  0.000260  0.000043  ...      0.0    0.000000     0.000014\n",
            "1  0.000017  0.000542  0.000017  ...      0.0    0.000017     0.000000\n",
            "2  0.000025  0.000787  0.000025  ...      0.0    0.000025     0.000000\n",
            "\n",
            "[3 rows x 15605 columns]\n",
            "{'': 0.47712125471966244, \"I'd\": 0.47712125471966244, 'rushed': 0.47712125471966244, 'combo': 0.47712125471966244, 'belief': 0.47712125471966244, 'props': 0.47712125471966244, 'claim': 0.47712125471966244, 'deficiencies': 0.47712125471966244, 'tubes': 0.47712125471966244, 'transmuted': 0.47712125471966244}\n",
            "\n",
            "\n",
            "                  I'd    rushed  ...  victory  proprietor  S-s-sahjunt\n",
            "0  0.000007  0.000124  0.000021  ...      0.0    0.000000     0.000007\n",
            "1  0.000008  0.000259  0.000008  ...      0.0    0.000008     0.000000\n",
            "2  0.000012  0.000375  0.000012  ...      0.0    0.000012     0.000000\n",
            "\n",
            "[3 rows x 15605 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGrgO1-qw4CM"
      },
      "source": [
        "Hamdaoui, Yassine. “TF(Term Frequency)-IDF(Inverse Document Frequency) from Scratch in Python&nbsp;.” Medium, Towards Data Science, 10 Dec. 2019, towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558. 26 Feb, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3FJtALIrbXk",
        "outputId": "7245776f-5434-4f5e-8442-cb40ca388aaf"
      },
      "source": [
        "#Step 4 - Similarity 20pts\n",
        "\n",
        "#Cosine Similarity \n",
        "from scipy.spatial import distance\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import sent_tokenize \n",
        "\n",
        "X_list= normalizeCorpus(stringOfOne)[:100]\n",
        "Y_list = normalizeCorpus(stringOfTwo)[:100]\n",
        "Z_list = normalizeCorpus(stringOfThree)[:100]\n",
        "\n",
        "def text_to_vector(text):\n",
        "    words = WORD.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "# sw contains the list of stopwords \n",
        "sw = nltk.corpus.stopwords.words('english')\n",
        "  \n",
        "# remove stop words from the string \n",
        "X_set = {w for w in X_list if not w in sw}  \n",
        "Y_set = {w for w in Y_list if not w in sw} \n",
        "Z_set = {w for w in Z_list if not w in sw} \n",
        "  \n",
        "# form a set containing keywords of both strings  \n",
        "rvectorx_y = X_set.union(Y_set)  \n",
        "rvectorx_z = X_set.union(Z_set)  \n",
        "rvectory_z = Y_set.union(Z_set)  \n",
        "print(rvectorx_x)\n",
        "print(rvectorx_y)\n",
        "print(rvectorx_z)\n",
        "def getCosineSimilarity(rvector, setOne, setTwo):\n",
        "  l1 =[];l2 =[] \n",
        "  for w in rvector: \n",
        "      if w in setOne: l1.append(1) # create a vector \n",
        "      else: l1.append(0) \n",
        "      if w in setTwo: l2.append(1) \n",
        "      else: l2.append(0) \n",
        "  c = 0\n",
        "    \n",
        "  # cosine formula  \n",
        "  for i in range(len(rvector)): \n",
        "          c+= l1[i]*l2[i] \n",
        "  cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "  return cosine \n",
        "\n",
        "XYCosine = getCosineSimilarity(rvectorx_y, X_set, Y_set)\n",
        "XZCosine = getCosineSimilarity(rvectorx_z, X_set, Z_set)\n",
        "YZCosine = getCosineSimilarity(rvectory_z, Y_set, Z_set) \n",
        "\n",
        "print(\"similarityXY: \", XYCosine) \n",
        "print(\"similarityXZ: \", XZCosine) \n",
        "print(\"similarityYZ: \", YZCosine) \n",
        "\n",
        "similarityMatrix= pd.DataFrame([XYCosine, XZCosine, YZCosine])\n",
        "print(similarityMatrix)\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'summer', 'awak', 'come', 'damag', 'told', 'morn', 'drone', 'lay', 'often', 'flash', 'dri', 'becom', 'monoton', 'get', 'anger', 'surfac', 'forget', 'adventur', 'defens', 'brennan', 'sometim', 'u', 'local', 'seem', 'quiet', 'marshal', 'hi', 'nurs', 'good', 'simpli', 'morgan', 'thing', 'patient', 'friend', 'antidot', 'could', 'go', 'poison', 'hanov', 'thought', 'sweet', 'day', 'woke', 'much', 'sell', 'disappoint', 'poignant', 'easiest', 'intern', 'state', 'gold', 'littl', 'appear', 'odor', 'gentl', 'live', 'back', 'dirti', 'ordinari', 'enough', 'way', 'ration', 'illinoi', 'night', 'certainli', 'hard', 'attend', 'lone', 'ill', 'charg', 'hot', 'smaller', 'troubl', 'spring', 'beneath', 'cook', 'knew', 'tire', 'window', 'hallucin', 'side', 'mari', 'dan', 'technic', 'plenti', 'ask', 'bitter', 'hospit', 'stubborn', 'ward', 'unusu', 'becaus', 'allow', 'stare', 'well', 'fetid', 'leav', 'newspap', 'left', 'countri', 'pretti', 'think', 'disturb', 'listen', 'smell', 'produc', 'dream', 'two', 'noth', 'eye', 'streak', 'found', 'revolv', 'grim', 'bu', 'field', 'jane', 'suddenli', 'best', 'address', 'long', 'plan', 'ann', 'counti', 'around', 'al', 'work', 'exhaust', 'stay', 'depress', 'mental', 'would', 'wife', 'middl', 'want', 'hurt', 'felt', 'west', 'le', 'marri', 'offset', 'compet', 'sleep', 'somewhat', 'incred', 'went', 'turner', 'chicago', 'budd', 'stream', 'fickl', 'rid', 'duller', 'voic', 'stori'}\n",
            "{'summer', 'awak', 'come', 'damag', 'told', 'morn', 'drone', 'lay', 'often', 'flash', 'dri', 'becom', 'monoton', 'get', 'anger', 'surfac', 'forget', 'adventur', 'defens', 'brennan', 'sometim', 'u', 'local', 'seem', 'quiet', 'marshal', 'hi', 'nurs', 'good', 'simpli', 'morgan', 'thing', 'patient', 'friend', 'antidot', 'could', 'go', 'poison', 'hanov', 'thought', 'sweet', 'day', 'woke', 'much', 'sell', 'disappoint', 'poignant', 'easiest', 'intern', 'state', 'gold', 'littl', 'appear', 'odor', 'gentl', 'live', 'back', 'dirti', 'ordinari', 'enough', 'way', 'ration', 'illinoi', 'night', 'certainli', 'hard', 'attend', 'lone', 'ill', 'charg', 'hot', 'smaller', 'troubl', 'spring', 'beneath', 'cook', 'knew', 'tire', 'window', 'hallucin', 'side', 'mari', 'dan', 'technic', 'plenti', 'ask', 'bitter', 'hospit', 'stubborn', 'ward', 'unusu', 'becaus', 'allow', 'stare', 'well', 'fetid', 'leav', 'newspap', 'left', 'countri', 'pretti', 'think', 'disturb', 'listen', 'smell', 'produc', 'dream', 'two', 'noth', 'eye', 'streak', 'found', 'revolv', 'grim', 'bu', 'field', 'jane', 'suddenli', 'best', 'address', 'long', 'plan', 'ann', 'counti', 'around', 'al', 'work', 'exhaust', 'stay', 'depress', 'mental', 'would', 'wife', 'middl', 'want', 'hurt', 'felt', 'west', 'le', 'marri', 'offset', 'compet', 'sleep', 'somewhat', 'incred', 'went', 'turner', 'chicago', 'budd', 'stream', 'fickl', 'rid', 'duller', 'voic', 'stori'}\n",
            "{'word', 'modern', 'demon', 'summer', 'awak', 'although', 'tire', 'distinct', 'make', 'dan', 'told', 'connot', 'real', 'disbelief', 'highli', 'plenti', 'ask', 'often', 'mean', 'though', 'dri', 'bitter', 'use', 'must', 'stubborn', 'ago', 'user', 'get', 'unusu', 'forget', 'becaus', 'allow', 'univers', 'stage', 'well', 'result', 'element', 'leav', 'sometim', 'countri', 'mani', 'think', 'contemporari', 'widespread', 'popular', 'produc', 'extent', 'still', 'dream', 'featur', 'understand', 'noth', 'hi', 'streak', 'found', 'revolv', 'sure', 'popul', 'taken', 'yet', 'centuri', 'usag', 'simpli', 'fairi', 'revis', 'morgan', 'thing', 'best', 'long', 'plan', 'ann', 'grant', 'antidot', 'around', 'al', 'could', 'work', 'transit', 'exhaust', 'term', 'never', 'poison', 'refer', 'stay', 'thought', 'day', 'correspond', 'woke', 'much', 'sell', 'disappoint', 'poignant', 'easiest', 'would', 'wife', 'middl', 'reject', 'want', 'neither', 'former', 'materi', 'confus', 'littl', 'hurt', 'matter', 'felt', 'spirit', 'le', 'common', 'back', 'basic', 'marri', 'ordinari', 'realiti', 'enough', 'axiomat', 'ration', 'context', 'world', 'educ', 'sleep', 'meant', 'one', 'night', 'certainli', 'hard', 'went', 'turner', 'direct', 'budd', 'stream', 'angel', 'mind', 'true', 'fickl', 'employ', 'rid', 'duller', 'experi', 'precis', 'tradit', 'hot', 'gener', 'entiti', 'smaller', 'almost', 'may', 'troubl', 'spring'}\n",
            "similarityXY:  0.07457013045372174\n",
            "similarityXZ:  0.026504600604023158\n",
            "similarityYZ:  0.012846934910327694\n",
            "          0\n",
            "0  0.074570\n",
            "1  0.026505\n",
            "2  0.012847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9cWWN-1u40M"
      },
      "source": [
        "Monacelli, Taylor. “Calculate Cosine Similarity given 2 Sentence Strings.” Stack Overflow, 20 Mar. 2020, stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings. 26 Feb, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr0Y6ZMjkjWl",
        "outputId": "b2b6202f-d1eb-4128-dea0-09bd40b1a897"
      },
      "source": [
        "#Similarity Matrix for Cosine Similarity \n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "X_list = normalizeCorpus(stringOfOne)[:100]\n",
        "Y_list = normalizeCorpus(stringOfTwo)[:100]\n",
        "Z_list = normalizeCorpus(stringOfThree)[:100]\n",
        "docs = X_list + Y_list + Z_list\n",
        "\n",
        "\n",
        "wordsOne = brown.words(categories='adventure')\n",
        "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\n",
        "                     use_idf=True, smooth_idf=True)\n",
        "feature_matrix = tv.fit_transform(docs)\n",
        "feature_matrix = feature_matrix.toarray()\n",
        "print(feature_matrix)\n",
        "\n",
        "similarity_matrix = cosine_similarity(feature_matrix)\n",
        "similarity_df = pd.DataFrame(similarity_matrix)\n",
        "print(similarity_df)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "     0    1    2    3    4    5    6    ...  293  294  295  296  297  298  299\n",
            "0    1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1    0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2    0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3    0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4    0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "295  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
            "296  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
            "297  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
            "298  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
            "299  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
            "\n",
            "[300 rows x 300 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-f7yV_DnyVU"
      },
      "source": [
        "Scrivner, Olga, Kumar, A . “CH04a - Feature Engineering Text Data - Traditional Strategies”, 27 Feb, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2av4rafo0BF",
        "outputId": "22dcb5c8-1c98-454c-8c7c-68f261723f83"
      },
      "source": [
        "#Perform Jaccard Similarity between 3 documents. Provide metrics between each document - 10pts\n",
        "from gensim.corpora import Dictionary\n",
        "from scipy.spatial import distance\n",
        "from gensim.matutils import jaccard\n",
        "\n",
        "texts = [\n",
        "   wordsOne, wordsTwo, wordsThree\n",
        "]\n",
        "\n",
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "distanceBetweenZeroandTwo = jaccard(corpus[0], corpus[2])\n",
        "distanceBetweenZeroandOne = jaccard(corpus[0], corpus[1])\n",
        "distanceBetweenOneandTwo = jaccard(corpus[1], corpus[2])\n",
        "print(\"\\nThe jaccard distance between the Text 0 and Text 2: \")\n",
        "print(distanceBetweenZeroandTwo)\n",
        "print(\"The jaccard distance between the Text 0 and Text 2 is greater than .5 thus are similar\\n\")\n",
        "print(\"The jaccard distance between the Text 0 and Text 1: \")\n",
        "print(distanceBetweenZeroandOne)\n",
        "print(\"The jaccard distance between the Text 0 and Text 1 is greater than .5 thus are similar\\n\")\n",
        "print(\"The jaccard distance between the Text 1 and Text 2:\" )\n",
        "print(distanceBetweenOneandTwo)\n",
        "print(\"The jaccard distance between the Text 1 and Text 2 is greater than .5 thus are similar\\n\")\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The jaccard distance between the Text 0 and Text 2: \n",
            "0.7561637284924729\n",
            "The jaccard distance between the Text 0 and Text 2 is greater than .5 thus are similar\n",
            "\n",
            "The jaccard distance between the Text 0 and Text 1: \n",
            "0.6281271984254333\n",
            "The jaccard distance between the Text 0 and Text 2 is greater than .5 thus are similar\n",
            "\n",
            "The jaccard distance between the Text 1 and Text 2:\n",
            "0.7386608400298236\n",
            "The jaccard distance between the Text 0 and Text 2 is greater than .5 thus are similar\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "BSyPsuZMuaz8",
        "outputId": "e7a42a59-c078-4922-bc8c-dc22745ac206"
      },
      "source": [
        "#Step 5 - Clustering 30pts\n",
        "#Perform Document clustering - 10pts\n",
        "#Use Cosine similarity matrix from step 4 and find Cosine Distance (see practice)\n",
        "#Use ward linkage for hierarchical clustering\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from scipy.cluster import  hierarchy\n",
        "from scipy.cluster.hierarchy import ward, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "X_list= normalizeCorpus(stringOfOne)[:100]\n",
        "Y_list = normalizeCorpus(stringOfTwo)[:100]\n",
        "Z_list = normalizeCorpus(stringOfThree)[:100]\n",
        "\n",
        "docsList = [\n",
        "   wordsOne, wordsTwo, wordsThree\n",
        "]\n",
        "docs = X_list + Y_list + Z_list\n",
        "\n",
        "#Use Cosine similarity matrix from step 4 and find Cosine Distance (see practice)\n",
        "#Use ward linkage for hierarchical clustering   \n",
        "cosine_distance = 1 - cosine_similarity(feature_matrix)\n",
        "linkage_matrix = ward(cosine_distance)\n",
        "print(cosine_distance)\n",
        "print(linkage_matrix)\n",
        "\n",
        "#Create a table with distance and cluster size (see lecture). Provide the output table - 10pts\n",
        "#Draw a dendrogram (see lecture and practice) - 10pts\n",
        "similarity_matrix = cosine_similarity(feature_matrix)\n",
        "Z = linkage(similarity_matrix, 'ward')\n",
        "\n",
        "Z = linkage(similarity_matrix, 'ward')\n",
        "tableWithDistanceandClusterSize = pd.DataFrame(Z, columns=['Document\\Cluster 1',\n",
        "'Document\\Cluster 2',\n",
        "'Distance', 'Cluster Size'], dtype=\"object\")\n",
        "print(tableWithDistanceandClusterSize)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('Distance')\n",
        "dendrogram(Z)\n",
        "plt.axhline(y=1.0, c=\"k\", ls=\"--\", lw=0.5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 0. 1. 1.]\n",
            " [1. 1. 1. ... 1. 0. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]]\n",
            "[[104.         115.           0.           2.        ]\n",
            " [116.         300.           0.           3.        ]\n",
            " [120.         301.           0.           4.        ]\n",
            " ...\n",
            " [318.         595.           5.62301885 292.        ]\n",
            " [360.         596.           5.62393599 296.        ]\n",
            " [357.         597.           5.62481631 300.        ]]\n",
            "    Document\\Cluster 1 Document\\Cluster 2 Distance Cluster Size\n",
            "0                  104                115        0            2\n",
            "1                  116                300        0            3\n",
            "2                  120                301        0            4\n",
            "3                  132                302        0            5\n",
            "4                  143                303        0            6\n",
            "..                 ...                ...      ...          ...\n",
            "294                324                593  4.22504          285\n",
            "295                353                594  4.22541          288\n",
            "296                318                595  5.62302          292\n",
            "297                360                596  5.62394          296\n",
            "298                357                597  5.62482          300\n",
            "\n",
            "[299 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gisYSengOnIE"
      },
      "source": [
        "From Lecture \n",
        "Scrivner, Olga, Kumar, A . “Part 2. Distance Metrics\n",
        "”, 27 Feb, 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxj2JsNwVUpX"
      },
      "source": [
        "Provide your brief conclusion:\n",
        "\n",
        "The brief conclusion is that there is major intersection over union in all the data sets based on the output of the Jaccard Similary. The cosine measure of similarity between two non-zero vectors of an inner product space shows similarity in their respective vectors. Doc 1 had the most similarity to the other docuemnts and doc 3 had the least similarity to the other documents is the interpretation from the results of the cosine similarity. Also seen from the dendogram is that at a larger size cluster the distance increases. Overall the conclusion can be made the documents have interpretable relationships.\n"
      ]
    }
  ]
}