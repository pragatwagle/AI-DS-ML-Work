{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02bea0b-d3e2-47b1-bd07-8ec6f40c7c27",
   "metadata": {},
   "source": [
    "**NOTE:** <br>\n",
    "For best results, please use the provided Docker container and run this notebook inside that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b188e-a62a-4b80-8c61-41e87f8f72bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homegrown Linear Regression (LR) via numpy\n",
    "\n",
    "Here, we will implement linear regression (a single layer network, with an architecture \"2-1\", corresponding to a model with 2 inputs, and a single output node) using array programming, and also in network fashion using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array class, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; but it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a single-layer network (linear regression) to random data by manually implementing the forward and backward (gradient descent) passes through the network using numpy operations. The following code shows how to accomplish this:\n",
    "\n",
    "\n",
    "The below code trains a linear regression model from random data (so the model learns nothing really; it is just an exercise).\n",
    "\n",
    "\n",
    "**QUESTION** :Run the below code and modify as needed and answer the following question. What is the value of the gradient for the first iteration of gradient descent after running the above code?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ec8ced-f056-4c66-82a7-a117fc48b237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, MSE: 2.389670625903901\n",
      "Epoch:0, Gradient: [[ 0.587]\n",
      " [-2.399]]\n",
      "--------------------\n",
      "Epoch:1, MSE: 2.383574809721735\n",
      "Epoch:1, Gradient: [[ 0.587]\n",
      " [-2.395]]\n",
      "--------------------\n",
      "Epoch:2, MSE: 2.377502693303068\n",
      "Epoch:2, Gradient: [[ 0.586]\n",
      " [-2.39 ]]\n",
      "--------------------\n",
      "Epoch:3, MSE: 2.3714541834963083\n",
      "Epoch:3, Gradient: [[ 0.585]\n",
      " [-2.385]]\n",
      "--------------------\n",
      "Epoch:4, MSE: 2.3654291875192457\n",
      "Epoch:4, Gradient: [[ 0.585]\n",
      " [-2.38 ]]\n",
      "--------------------\n",
      "Epoch:5, MSE: 2.359427612957574\n",
      "Epoch:5, Gradient: [[ 0.584]\n",
      " [-2.375]]\n",
      "--------------------\n",
      "Epoch:6, MSE: 2.3534493677634245\n",
      "Epoch:6, Gradient: [[ 0.583]\n",
      " [-2.371]]\n",
      "--------------------\n",
      "Epoch:7, MSE: 2.3474943602538985\n",
      "Epoch:7, Gradient: [[ 0.583]\n",
      " [-2.366]]\n",
      "--------------------\n",
      "Epoch:8, MSE: 2.3415624991096156\n",
      "Epoch:8, Gradient: [[ 0.582]\n",
      " [-2.361]]\n",
      "--------------------\n",
      "Epoch:9, MSE: 2.3356536933732595\n",
      "Epoch:9, Gradient: [[ 0.581]\n",
      " [-2.357]]\n",
      "--------------------\n",
      "Weights after training: ,[[ 0.208]\n",
      " [-1.222]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# m_rows is batch size; \n",
    "# D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 2, 1\n",
    "#m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "np.random.seed(seed=42) #fix the seed\n",
    "# Create random input and output data\n",
    "X_train = np.random.randn(m_rows, D_in)\n",
    "y_train = np.random.randn(m_rows, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "W1 = np.random.randn(D_in, D_out) #[w0, w1, w2, ...w999]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "for epoch in range(10):  #Gradient descent\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = X_train.dot(W1)\n",
    "    #y_pred = X_train @ W1 \n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y_train).mean()\n",
    "    print(f\"Epoch:{epoch}, MSE: {loss}\")\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    \n",
    "    # Pragat Question: 2 seems arbitary, is there a reason 2 was chosen?\n",
    "    \n",
    "    err = 2.0 * (y_pred - y_train)\n",
    "    grad_W1 = X_train.T.dot(err)/m_rows  #weighted sum of the train data\n",
    "    print(f\"Epoch:{epoch}, Gradient: {np.round(grad_W1, 3)}\")\n",
    "    \n",
    "    # Update weights via gradient descent\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    print('--------------------')\n",
    "    \n",
    "print(f\"Weights after training: ,{np.round(W1, 3)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773e621-a5b0-42ec-b7c5-0525c0056196",
   "metadata": {},
   "source": [
    "## Linear regression using tensors and autograd\n",
    "\n",
    "\n",
    "Traditionally, say using Numpy, we had to manually implement both the forward and backward passes to train a  linear regression model (aka a single-layered neural network). Manually implementing the backward pass is not a big deal for a linear regression model or for a small/shallow network, but this can quickly get very tricky for larger multilayers networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality.\n",
    "\n",
    "When using autograd,\n",
    "* **the forward pass of your network (code for making a prediction) will define a computational graph;**\n",
    "* nodes in the graph will be Tensors,\n",
    "* and edges will be functions that produce output Tensors from input Tensors.\n",
    "\n",
    "PyTorch builds up a graph as you compute the forward pass through the network, and one call to **backward()** on some result node (loss function) then augments each intermediate node in the graph with the gradient of the result node with respect to that intermediate node.\n",
    "\n",
    "This sounds complicated, but it’s pretty simple to use in practice. We wrap our PyTorch Tensors in Variable objects; a Variable represents a node in a computational graph. If x is a Variable then x.data is a Tensor, and x.grad is another Variable holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "PyTorch Variables are PyTorch Tensors variables.\n",
    "\n",
    "Here we use PyTorch tensor variables and autograd to implement our single-layer network (linear regression model); now we no longer need to manually implement the backward pass through the network:\n",
    "\n",
    "**Question** Run the below code and modify as needed to get the value of the first element of the learnt linear regression model. What is the value of the first element of the learnt linear regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3960f26-95a9-4650-894c-0379f1c59a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10, MSE: 829.467\n",
      "Epoch:20, MSE: 828.909\n",
      "Epoch:30, MSE: 828.351\n",
      "Epoch:40, MSE: 827.794\n",
      "Epoch:50, MSE: 827.237\n",
      "The first value of the learnt linear regression model weight\n",
      "tensor([1.8839], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X_train = torch.randn((m_rows, D_in), requires_grad=False) #use (m_rows, D_in) or m_rows, D_in\n",
    "Y_train = torch.randn(m_rows, D_out, requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "W1 = torch.randn(D_in, D_out,  requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(50):\n",
    "    # Forward pass: compute predicted y using operations on Variables; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    Y_pred = X_train.matmul(W1)\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "    loss = (Y_pred - Y_train).pow(2).mean()\n",
    "    if (epoch +1 )%10 ==0:  #print every 10 epochs\n",
    "        print(f\"Epoch:{epoch+1}, MSE: {loss.data.numpy():.6}\")\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call W1.grad  will be Variables holding the gradient\n",
    "    # of the loss with respect to W1.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent; W1.data and W2.data are Tensors,\n",
    "    # W1.grad are Variables and W1.grad.data aare\n",
    "    # Tensors.\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    # Manually zero the gradients after updating weights\n",
    "    W1.grad.data.zero_()\n",
    "\n",
    "print('The first value of the learnt linear regression model weight')    \n",
    "# print the first value of the learnt linear regression model weight\n",
    "\n",
    "# TODO: Add code below inside the print statement\n",
    "print(W1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a0079-a4e7-4e12-a870-814e10ca9442",
   "metadata": {},
   "source": [
    "# PyTorch: optim\n",
    "\n",
    "\n",
    "Up to this point we have updated the weights of our models by manually mutating the .data member for Variables holding learnable parameters.\n",
    "\n",
    "`W1.data -= learning_rate * W1.grad.data`\n",
    "\n",
    "This is not a huge burden for simple optimization algorithms like stochastic gradient descent on a single layer network (such as a multiple linear regession (MLR) model), but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc. More on these optimizers later.\n",
    "\n",
    "The **optim** package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.\n",
    "\n",
    "The following example brings these two highly scaleable concepts of computational graphs and optimization to life:\n",
    "\n",
    "* we will use the nn package to define our our linear regression module,\n",
    "* and we will use optimize the model using the **Adam** algorithm, a variant of stochastic gradient descent. The **Adam** algorithm comes as part of the the **optim** package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be74e30-04bd-4526-98ba-9504a8406d70",
   "metadata": {},
   "source": [
    "# Building PyTorch Networks using the Sequential API\n",
    "\n",
    "The **Sequential** class allows us to build a neural network in PyTorch in a high-level quick and modular manner.\n",
    "\n",
    "* The Sequential class allows us to build PyTorch neural networks on-the-fly without having to build an explicit class.\n",
    "* This make it much easier to rapidly build networks and allows us to skip over the step where we implement the forward() method. When we use the sequential way of building a PyTorch network, we construct the forward() method implicitly by defining our network's architecture sequentially.\n",
    " \n",
    "\n",
    "Here we use the Sequential() API to build a single-layered neural network (upon closer inspection you see it is a linear regression model) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82d3513-7873-4d25-bd5f-83b30b175e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, MSE: 1.48583198\n",
      "Epoch:10, MSE: 1.24699199\n",
      "Epoch:20, MSE: 1.03472435\n",
      "Epoch:30, MSE: 0.849931657\n",
      "Epoch:40, MSE: 0.691670835\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X_train = torch.randn((m_rows, D_in), requires_grad=False) #use (m_rows, D_in) or m_rows, D_in\n",
    "Y_train = torch.randn(m_rows, D_out, requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "# NOT Need as we are using the nn package\n",
    "#  W1 = torch.randn(D_in, D_out,  requires_grad=True)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API makes things simple\n",
    "model = torch.nn.Sequential(  #  X_train @ W1\n",
    "    torch.nn.Linear(D_in, D_out),   # X.matmul(W1)\n",
    ")\n",
    "# loss scaffolding layer\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(50):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Y_train)\n",
    "    if epoch % 10 == 0:\n",
    "         print(f\"Epoch:{epoch}, MSE: {loss.item():.9}\")\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4281d-4b27-430b-88ae-fdf0a530e14f",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "A single layered  neural network consisting of an input layer (not counted as a layer) and an output layer  is essentially just a linear regression model. A two-layered network consisting for an input layer (not counted as a layer), a hidden layer, and an output layer is still essentially just a linear regression model. Both these networks are just linear transformations of the inputs. To make these networks nonlinear (and enhance their predictive power), we introduce  a non-linear activation function (that acts element-wise on the linear transformed inputs).\n",
    "\n",
    " \n",
    "\n",
    "A popular activation function is the Sigmoid function. It is one of the most widely used non-linear activation function. Sigmoid transforms the values between the range 0 and 1. Here is the mathematical expression for sigmoid:\n",
    "\n",
    "`f(x) = 1/(1+e^-x)`\n",
    "\n",
    "``` import numpy as np\n",
    "def sigmoid_function(x):\n",
    "    z = (1/(1 + np.exp(-x)))\n",
    "    return z\n",
    "```  \n",
    "    \n",
    "**ReLU**\n",
    "\n",
    "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. This means that the neurons will only be deactivated if the output of the linear transformation is less than 0. \n",
    "\n",
    "```\n",
    "def relu_function(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "```\n",
    "\n",
    "`relu_function(7), relu_function(-7). # returns (7, 0) as a result`\n",
    "\n",
    "For the negative part of the input domain, you will notice that the value is zero.\n",
    "\n",
    "In PyTorch ReLU is available as a built in layer via  torch.nn.ReLU().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10745a70-4023-48de-a52b-e67f4ef50e5b",
   "metadata": {},
   "source": [
    "**The following are examples of neural networks consisting of input, hidden, and output layers:**\n",
    "\n",
    "```    \n",
    "network1 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(out_features, 1) # X.matmul(W2) + b2 )\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "network2 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.Linear(out_features, 1) # X.matmul(W2) + b2 )\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "network3 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(in_features, out_features), # X.matmul(W2) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(out_features, 1) # X.matmul(W3) + b2 )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da53625-3312-45fe-ad11-2f75a14c340d",
   "metadata": {},
   "source": [
    "# Boston house price regression via Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19c8e00-fbb0-49f2-baa7-36cdee912f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /Users/pragatwagle/opt/anaconda3/lib/python3.9/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b63bb7ff-4e9e-4360-9fbe-ef5571d9f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary  #install it if necessary using !pip install torchsummary \n",
    "import torch\n",
    "#import torchvision\n",
    "import torch.utils.data\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for better reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# if GPU, use cuda-enabled GPU device else CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "# X.shape\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation) #Transform test set with the same constants\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_validation_tensor = torch.from_numpy(X_validation)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "y_validation_tensor = torch.from_numpy(y_validation)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "boston_validation = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# create dataloader\n",
    "\n",
    "\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 16\n",
    "\n",
    "trainloader_boston = torch.utils.data.DataLoader(boston_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "validloader_boston = torch.utils.data.DataLoader(boston_validation, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_boston = torch.utils.data.DataLoader(boston_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_boston_seq_model(\n",
    "    D_hidden=20,  \n",
    "    hidden_layers_count=1,\n",
    "    opt=optim.SGD,\n",
    "    epochs=5\n",
    "):\n",
    "    \n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 1  # Output layer neurons - depend on what you're trying to predict, here, just a single value\n",
    "    \n",
    "    arch_string = f\"{D_in}-\"\n",
    "    arch_string += f\"{D_hidden}-\" * hidden_layers_count + f\"{D_out}\"\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, D_hidden),  # X.matmul(W1)\n",
    "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for _ in range(hidden_layers_count):\n",
    "        layers.append(torch.nn.Linear(D_hidden, D_hidden))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(D_hidden, D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    # MSE loss scaffolding layer\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = opt(model.parameters(), lr=0.0001)\n",
    "\n",
    "    print('-'*50)\n",
    "    print('Model summary:')\n",
    "    print(model)\n",
    "    summary(model, (1, 13))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Print device used - CPU or GPU\n",
    "    print(f\"Using {device}...\")\n",
    "\n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for batch, data in enumerate(trainloader_boston):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # do forward pass\n",
    "            output = model(inputs.float())\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "            # get gradients w.r.t to parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform gradient update\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1},Train MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    print('-'*50)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for batch, data in enumerate(validloader_boston):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)\n",
    "        # do forward pass\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        # compute loss and gradients\n",
    "        loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "    valid_mse = np.round(running_loss / count, 3)\n",
    "    print(f\" Validation  MSE loss: {valid_mse}\")\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for batch, data in enumerate(testloader_boston):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)\n",
    "        # do forward pass\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        # compute loss and gradients\n",
    "        loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "    test_mse = np.round(running_loss / count, 3)\n",
    "    print(f\" TEST  MSE loss: {test_mse}\")\n",
    "    \n",
    "    return arch_string, valid_mse, test_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cb2c668-1e34-46ee-bbdf-44f9d4b09519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model summary:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=13, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=13, out_features=13, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=13, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 13]             182\n",
      "              ReLU-2                [-1, 1, 13]               0\n",
      "            Linear-3                [-1, 1, 13]             182\n",
      "              ReLU-4                [-1, 1, 13]               0\n",
      "            Linear-5                 [-1, 1, 1]              14\n",
      "================================================================\n",
      "Total params: 378\n",
      "Trainable params: 378\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Using cpu...\n",
      "Epoch 1,Train MSE loss: 587.908\n",
      "Epoch 2,Train MSE loss: 582.492\n",
      "Epoch 3,Train MSE loss: 584.462\n",
      "Epoch 4,Train MSE loss: 583.428\n",
      "Epoch 5,Train MSE loss: 580.782\n",
      "Epoch 6,Train MSE loss: 579.755\n",
      "Epoch 7,Train MSE loss: 574.352\n",
      "Epoch 8,Train MSE loss: 573.548\n",
      "Epoch 9,Train MSE loss: 574.138\n",
      "Epoch 10,Train MSE loss: 565.947\n",
      "Finished Training\n",
      "--------------------------------------------------\n",
      " Validation  MSE loss: 566.058\n",
      " TEST  MSE loss: 513.353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Valid MSE loss</th>\n",
       "      <th>Test MSE loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13-20-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>535.371</td>\n",
       "      <td>530.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13-5-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>522.416</td>\n",
       "      <td>529.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13-10-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>611.81</td>\n",
       "      <td>554.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13-13-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>579.545</td>\n",
       "      <td>526.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13-3-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>977.47</td>\n",
       "      <td>552.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13-13-1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>566.058</td>\n",
       "      <td>513.353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                        Optimizer Epochs Valid MSE loss  \\\n",
       "0             13-20-1  <class 'torch.optim.adam.Adam'>      5        535.371   \n",
       "1              13-5-1  <class 'torch.optim.adam.Adam'>     10        522.416   \n",
       "2             13-10-1  <class 'torch.optim.adam.Adam'>     10         611.81   \n",
       "3             13-13-1  <class 'torch.optim.adam.Adam'>     10        579.545   \n",
       "4              13-3-1  <class 'torch.optim.adam.Adam'>     10         977.47   \n",
       "5             13-13-1    <class 'torch.optim.sgd.SGD'>     10        566.058   \n",
       "\n",
       "  Test MSE loss  \n",
       "0       530.214  \n",
       "1       529.707  \n",
       "2       554.839  \n",
       "3       526.596  \n",
       "4       552.191  \n",
       "5       513.353  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve a low MSE value\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(D_hidden) - The number of neurons in each hidden layer, DEFAULT: 20\n",
    "(hidden_layers_count) - The number of hidden layers in the network,  DEFAULT: 1\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "'''\n",
    "\n",
    "D_hidden = 13\n",
    "hidden_layers_count = 1\n",
    "opt = optim.SGD   # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 10\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, valid_mse, test_mse = run_boston_seq_model(\n",
    "    D_hidden,\n",
    "    hidden_layers_count,\n",
    "    opt,\n",
    "    epochs\n",
    ")\n",
    "    \n",
    "try: bostonLog \n",
    "except : bostonLog  = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Valid MSE loss\",\n",
    "        \"Test MSE loss\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "bostonLog.loc[len(bostonLog)] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{valid_mse}\", \n",
    "    f\"{test_mse}\",\n",
    "]\n",
    "\n",
    "bostonLog "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b1a6c-d008-4dab-ae54-f0896b360a73",
   "metadata": {},
   "source": [
    "# Perform Classification on Iris dataset via Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f40c1042-89f0-4f43-810e-6714220a031e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn import datasets\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True)\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "iris_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "iris_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# print(X_test.shape)\n",
    "\n",
    "# create dataloader\n",
    "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 16\n",
    "trainloader_iris = torch.utils.data.DataLoader(iris_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_iris = torch.utils.data.DataLoader(iris_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_iris_model(\n",
    "    D_hidden=50,  \n",
    "    hidden_layers_count=1,\n",
    "    opt=optim.SGD,\n",
    "    epochs=5\n",
    "):\n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 3  # Output layer neurons - depend on what you're trying to predict, here, 3 classes\n",
    "    \n",
    "    arch_string = f\"{D_in}-\"\n",
    "    arch_string += f\"{D_hidden}-\" * hidden_layers_count + f\"{D_out}\"\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, D_hidden),  # X.matmul(W1)\n",
    "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for _ in range(hidden_layers_count):\n",
    "        layers.append(torch.nn.Linear(D_hidden, D_hidden))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(D_hidden, D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # use Cross Entropy and SGD optimizer.\n",
    "    loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
    "    optimizer = opt(model.parameters(), lr=0.01)\n",
    "\n",
    "    #summary(model, (4, 20))\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    print('-'*50)\n",
    "\n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "    train_losses = {}\n",
    "    train_accuracy = {}\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = []\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        for batch, data in enumerate(trainloader_iris):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # do forward pass\n",
    "            output = model(inputs.float())\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(output, target)\n",
    "            # get gradients w.r.t to parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform gradient update\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred.extend(torch.argmax(output, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        epoch_training_loss = np.mean(running_loss)\n",
    "        train_losses[epoch+1] = epoch_training_loss\n",
    "        print(f\"Epoch {epoch+1}, Training loss: {np.round(epoch_training_loss, 3)}\")\n",
    "\n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum()/ correct.size\n",
    "        train_accuracy[epoch+1] = accuracy\n",
    "        print(f\"Epoch {epoch+1}, Training accuracy: {np.round(accuracy, 3)}\")\n",
    "\n",
    "    print(train_losses)\n",
    "\n",
    "    print('Finished Training')\n",
    "    print('-'*50)\n",
    "\n",
    "    test_batch_losses = []\n",
    "    test_y_pred = []\n",
    "    test_target = []\n",
    "    for batch, data in enumerate(testloader_iris):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)\n",
    "        # do forward pass\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        test_batch_losses.append(loss.item())\n",
    "\n",
    "        test_y_pred.extend(torch.argmax(output, dim=1).tolist())\n",
    "        test_target.extend(target.tolist())\n",
    "\n",
    "    print(f\"Test loss: {np.round(np.mean(test_batch_losses), 3)}\")\n",
    "\n",
    "    #accuracy\n",
    "    test_correct = (np.array(test_y_pred) == np.array(test_target))\n",
    "    test_accuracy = test_correct.sum()/ test_correct.size\n",
    "    test_accuracy = np.round(test_accuracy, 3)\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    \n",
    "    return arch_string, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "143f5f73-58c4-45c5-9d5a-0eeb9e22c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Epoch 1, Training loss: 1.157\n",
      "Epoch 1, Training accuracy: 0.362\n",
      "Epoch 2, Training loss: 1.073\n",
      "Epoch 2, Training accuracy: 0.362\n",
      "Epoch 3, Training loss: 1.121\n",
      "Epoch 3, Training accuracy: 0.352\n",
      "Epoch 4, Training loss: 1.111\n",
      "Epoch 4, Training accuracy: 0.352\n",
      "Epoch 5, Training loss: 1.103\n",
      "Epoch 5, Training accuracy: 0.352\n",
      "Epoch 6, Training loss: 1.109\n",
      "Epoch 6, Training accuracy: 0.352\n",
      "Epoch 7, Training loss: 1.093\n",
      "Epoch 7, Training accuracy: 0.352\n",
      "Epoch 8, Training loss: 1.14\n",
      "Epoch 8, Training accuracy: 0.352\n",
      "Epoch 9, Training loss: 1.112\n",
      "Epoch 9, Training accuracy: 0.352\n",
      "Epoch 10, Training loss: 1.098\n",
      "Epoch 10, Training accuracy: 0.352\n",
      "Epoch 11, Training loss: 1.117\n",
      "Epoch 11, Training accuracy: 0.352\n",
      "Epoch 12, Training loss: 1.099\n",
      "Epoch 12, Training accuracy: 0.352\n",
      "Epoch 13, Training loss: 1.105\n",
      "Epoch 13, Training accuracy: 0.352\n",
      "Epoch 14, Training loss: 1.102\n",
      "Epoch 14, Training accuracy: 0.352\n",
      "Epoch 15, Training loss: 1.099\n",
      "Epoch 15, Training accuracy: 0.352\n",
      "Epoch 16, Training loss: 1.097\n",
      "Epoch 16, Training accuracy: 0.352\n",
      "Epoch 17, Training loss: 1.07\n",
      "Epoch 17, Training accuracy: 0.352\n",
      "Epoch 18, Training loss: 1.105\n",
      "Epoch 18, Training accuracy: 0.352\n",
      "Epoch 19, Training loss: 1.075\n",
      "Epoch 19, Training accuracy: 0.352\n",
      "Epoch 20, Training loss: 1.084\n",
      "Epoch 20, Training accuracy: 0.352\n",
      "Epoch 21, Training loss: 1.106\n",
      "Epoch 21, Training accuracy: 0.352\n",
      "Epoch 22, Training loss: 1.11\n",
      "Epoch 22, Training accuracy: 0.352\n",
      "Epoch 23, Training loss: 1.119\n",
      "Epoch 23, Training accuracy: 0.352\n",
      "Epoch 24, Training loss: 1.077\n",
      "Epoch 24, Training accuracy: 0.352\n",
      "Epoch 25, Training loss: 1.069\n",
      "Epoch 25, Training accuracy: 0.352\n",
      "Epoch 26, Training loss: 1.075\n",
      "Epoch 26, Training accuracy: 0.352\n",
      "Epoch 27, Training loss: 1.096\n",
      "Epoch 27, Training accuracy: 0.352\n",
      "Epoch 28, Training loss: 1.084\n",
      "Epoch 28, Training accuracy: 0.352\n",
      "Epoch 29, Training loss: 1.092\n",
      "Epoch 29, Training accuracy: 0.352\n",
      "Epoch 30, Training loss: 1.105\n",
      "Epoch 30, Training accuracy: 0.352\n",
      "{1: 1.1568253636360168, 2: 1.0727641582489014, 3: 1.1212197542190552, 4: 1.1113357543945312, 5: 1.1032023429870605, 6: 1.1090389490127563, 7: 1.0926401019096375, 8: 1.140015184879303, 9: 1.1120612025260925, 10: 1.0981236696243286, 11: 1.1170452237129211, 12: 1.099224328994751, 13: 1.1054935455322266, 14: 1.101723849773407, 15: 1.099276602268219, 16: 1.0967010259628296, 17: 1.0696001648902893, 18: 1.1054905652999878, 19: 1.0751270055770874, 20: 1.083653450012207, 21: 1.1058061122894287, 22: 1.1104264855384827, 23: 1.118853211402893, 24: 1.0772311091423035, 25: 1.0692603588104248, 26: 1.075347363948822, 27: 1.096471130847931, 28: 1.0836514234542847, 29: 1.0924298763275146, 30: 1.1048812866210938}\n",
      "Finished Training\n",
      "--------------------------------------------------\n",
      "Test loss: 1.086\n",
      "Test accuracy: 0.289\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4-10-3</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>77.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4-20-3</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>91.10000000000001%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4-10-3</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>91.10000000000001%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4-20-3</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>86.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4-10-10-3</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>71.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4-10-10-3</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>82.19999999999999%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4-10-10-3</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>28.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4-20-20-3</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>30</td>\n",
       "      <td>28.9%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                        Optimizer Epochs  \\\n",
       "0              4-10-3  <class 'torch.optim.adam.Adam'>     20   \n",
       "1              4-20-3  <class 'torch.optim.adam.Adam'>     20   \n",
       "2              4-10-3  <class 'torch.optim.adam.Adam'>     10   \n",
       "3              4-20-3  <class 'torch.optim.adam.Adam'>     10   \n",
       "4           4-10-10-3  <class 'torch.optim.adam.Adam'>     10   \n",
       "5           4-10-10-3  <class 'torch.optim.adam.Adam'>     20   \n",
       "6           4-10-10-3    <class 'torch.optim.sgd.SGD'>     20   \n",
       "7           4-20-20-3    <class 'torch.optim.sgd.SGD'>     30   \n",
       "\n",
       "        Test accuracy  \n",
       "0               77.8%  \n",
       "1  91.10000000000001%  \n",
       "2  91.10000000000001%  \n",
       "3               86.7%  \n",
       "4               71.1%  \n",
       "5  82.19999999999999%  \n",
       "6               28.9%  \n",
       "7               28.9%  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve larger train/test accuracy\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(D_hidden) - The number of neurons in each hidden layer, DEFAULT: 20\n",
    "(hidden_layers_count) - The number of hidden layers in the network,  DEFAULT: 1\n",
    "(opt) - The opt function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "'''\n",
    "\n",
    "D_hidden = 20\n",
    "hidden_layers_count = 2\n",
    "opt = optim.SGD  # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 30\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, test_accuracy = run_iris_model(\n",
    "    D_hidden,\n",
    "    hidden_layers_count,\n",
    "    opt,\n",
    "    epochs\n",
    ")\n",
    "    \n",
    "\n",
    "try: irisLog \n",
    "except : irisLog = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Test accuracy\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "irisLog.loc[len(irisLog)] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{test_accuracy * 100}%\",\n",
    "]\n",
    "\n",
    "irisLog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9f920-8f12-4b1b-b701-2f4ce24740a2",
   "metadata": {},
   "source": [
    "# Boston house price regression via OOP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0d2e890-f784-405f-a897-4e661c7d73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 13) (76, 13)\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary  #install it if necessary using !pip install torchsummary \n",
    "import torch\n",
    "#import torchvision\n",
    "import torch.utils.data\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train =      scaler.fit_transform(X_train).astype(float)\n",
    "X_validation = scaler.transform(X_validation).astype(float) #Transform valid set with the same constants\n",
    "X_test =       scaler.transform(X_test).astype(float)       #Transform test  set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_validation_tensor = torch.from_numpy(X_validation).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "y_validation_tensor = torch.from_numpy(y_validation).float()\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "train_ds = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "validation_ds = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "test_ds = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "\n",
    "print(X_validation.shape, X_test.shape)\n",
    "\n",
    "# create dataloader\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(validation_ds, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_boston_oop_model(\n",
    "    D_hidden=20,  \n",
    "    opt=optim.SGD,\n",
    "    epochs=5\n",
    "):\n",
    "    \n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 1  # Output layer neurons - depend on what you're trying to predict, here, just a single value\n",
    "    \n",
    "    arch_string = f\"{D_in}-\"\n",
    "    arch_string += f\"{D_hidden}-\" * 1 + f\"{D_out}\"\n",
    "\n",
    "    # Use the OOP API to define a deep neural network model\n",
    "    #\n",
    "    class BaseModel(nn.Module):\n",
    "        \"\"\"Custom module for a simple  regressor\"\"\"\n",
    "        def __init__(self, in_features, size_hidden=10, n_output=1):\n",
    "            super(BaseModel, self).__init__()\n",
    "            self.fc1 = torch.nn.Linear(in_features, size_hidden)   # hidden layer\n",
    "            self.fc2 = torch.nn.Linear(size_hidden, n_output)      # output layer\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            x = F.relu(self.fc1(x))   # activation function for hidden layer\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Print device used - CPU or GPU\n",
    "    print(f\"Using {device}...\")\n",
    "\n",
    "    # create classifier and optimizer objects\n",
    "    model = BaseModel(in_features=D_in, size_hidden = D_hidden, n_output = D_out)\n",
    "    model.to(device) # put on GPU before setting up the optimizer\n",
    "\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    summary(model, (1, 13))\n",
    "    print('-'*50)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = opt(model.parameters(), lr=0.0001)\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        # dataset API gives us pythonic batching \n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)        \n",
    "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = model(inputs) #prediction over the input data\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(preds, target)    #mean loss for this batch\n",
    "\n",
    "            loss.backward() #calculate nabla_w\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()  #update W\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        print(f\"Train MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "\n",
    "\n",
    "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        for i,data in enumerate(data_loader):\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)                \n",
    "            outputs = model(inputs)      \n",
    "\n",
    "            loss = loss_fn(outputs, targets)           # compute loss value\n",
    "\n",
    "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "            count += 1\n",
    "\n",
    "        # compute mean loss\n",
    "        mean_loss = np.round(overall_loss/count, 3)\n",
    "        print(f\"{tag} MSE loss: {mean_loss:.3f}\")\n",
    "        return mean_loss\n",
    "        \n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train_epoch(epoch, model, loss_fn, optimizer, train_loader)\n",
    "        evaluate_model(epoch, model, loss_fn, optimizer, valid_loader, tag = \"Validation\")\n",
    "    print(\"-\"*50)\n",
    "    test_mse = evaluate_model(epoch, model, loss_fn, opt, test_loader, tag=\"Test\")\n",
    "    \n",
    "    return arch_string, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "880d6090-48dd-4647-b232-0e2c10d473d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu...\n",
      "--------------------------------------------------\n",
      "Model:\n",
      "BaseModel(\n",
      "  (fc1): Linear(in_features=13, out_features=3, bias=True)\n",
      "  (fc2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1, 3]              42\n",
      "            Linear-2                 [-1, 1, 1]               4\n",
      "================================================================\n",
      "Total params: 46\n",
      "Trainable params: 46\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Epoch 1\n",
      "Train MSE loss: 618.999\n",
      "Validation MSE loss: 599.502\n",
      "Epoch 2\n",
      "Train MSE loss: 616.245\n",
      "Validation MSE loss: 690.246\n",
      "Epoch 3\n",
      "Train MSE loss: 617.139\n",
      "Validation MSE loss: 568.488\n",
      "Epoch 4\n",
      "Train MSE loss: 617.964\n",
      "Validation MSE loss: 562.689\n",
      "Epoch 5\n",
      "Train MSE loss: 617.988\n",
      "Validation MSE loss: 731.151\n",
      "Epoch 6\n",
      "Train MSE loss: 618.247\n",
      "Validation MSE loss: 559.095\n",
      "Epoch 7\n",
      "Train MSE loss: 619.465\n",
      "Validation MSE loss: 985.483\n",
      "Epoch 8\n",
      "Train MSE loss: 621.395\n",
      "Validation MSE loss: 554.728\n",
      "Epoch 9\n",
      "Train MSE loss: 614.387\n",
      "Validation MSE loss: 560.234\n",
      "Epoch 10\n",
      "Train MSE loss: 614.467\n",
      "Validation MSE loss: 583.651\n",
      "--------------------------------------------------\n",
      "Test MSE loss: 552.595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Test MSE loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13-20-1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>519.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13-13-1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>517.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13-10-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>543.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13-20-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>536.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13-13-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>533.539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13-6-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>528.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13-3-1</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>552.595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                        Optimizer Epochs Test MSE loss\n",
       "0             13-20-1    <class 'torch.optim.sgd.SGD'>     10       519.029\n",
       "1             13-13-1    <class 'torch.optim.sgd.SGD'>     10       517.861\n",
       "2             13-10-1  <class 'torch.optim.adam.Adam'>     10       543.553\n",
       "3             13-20-1  <class 'torch.optim.adam.Adam'>     10       536.369\n",
       "4             13-13-1  <class 'torch.optim.adam.Adam'>     10       533.539\n",
       "5              13-6-1  <class 'torch.optim.adam.Adam'>     10       528.066\n",
       "6              13-3-1  <class 'torch.optim.adam.Adam'>     10       552.595"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve a low MSE value\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(D_hidden) - The number of neurons in each hidden layer, DEFAULT: 20\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "'''\n",
    "\n",
    "D_hidden = 3\n",
    "opt = optim.Adam  # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 10\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, test_mse = run_boston_oop_model(\n",
    "    D_hidden,\n",
    "    opt,\n",
    "    epochs\n",
    ")\n",
    "    \n",
    "try: bostonOopLog \n",
    "except : bostonOopLog  = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Test MSE loss\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "bostonOopLog.loc[len(bostonOopLog )] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{test_mse}\",\n",
    "]\n",
    "\n",
    "bostonOopLog "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50556ec6-d85c-43cb-81e9-da82fba243f6",
   "metadata": {},
   "source": [
    "# Perform Classification on HCDR dataset via Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85026e-5625-47a8-b74f-6523bd195527",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Note:\n",
    "Have a look at the Home Credit Default Risk data on Kaggle: https://www.kaggle.com/competitions/home-credit-default-risk/overview <br>\n",
    "\n",
    "There are multiple .csv files provided for this challenge on Kaggle. For this question, we'll be only using the **application_train.csv** to predict TARGET : 1 or 0. <br>\n",
    "\n",
    "**DATASET LINK:** Download the .zip of the .csv file from here: https://www.dropbox.com/s/rmqsrhhc8qhyq50/application_train.csv.zip?dl=0\n",
    "\n",
    "Save the **application_train.csv.zip** in the same directory you're running this notebook in. Please ensure that the .zip is downloaded and then run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac06dc7b-a73b-4660-9237-d731d1bae92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime that supports apt.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!apt install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b78c6265-bb89-4472-8db4-b1d08e18d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  application_train.csv.zip\n",
      "  inflating: application_train.csv   \n"
     ]
    }
   ],
   "source": [
    "!unzip -o application_train.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80d60d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (307511, 121) (307511,)\n",
      "(215257, 245) (215257,) (92254, 245) (92254,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "hcdr_application = pd.read_csv(\"application_train.csv\")\n",
    "X = hcdr_application.drop('TARGET', axis = 1)\n",
    "y = hcdr_application.TARGET\n",
    "print(\"Shapes:\", X.shape, y.shape)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True)\n",
    "\n",
    "## Scaling\n",
    "numerical_features = X.select_dtypes(include = ['int64','float64']).columns\n",
    "numerical_features = numerical_features.tolist()\n",
    "\n",
    "num_pipeline =Pipeline([('std',StandardScaler()),\n",
    "        ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_features = X.select_dtypes(include = ['object']).columns\n",
    "categorical_features = categorical_features.tolist()\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "features = numerical_features + categorical_features\n",
    "\n",
    "data_pipeline = ColumnTransformer([\n",
    "       (\"num_pipeline\", num_pipeline, numerical_features),\n",
    "       (\"cat_pipeline\", cat_pipeline, categorical_features)],\n",
    "        remainder='drop',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "X_train = data_pipeline.fit_transform(X_train)\n",
    "X_test = data_pipeline.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "hcdr_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "hcdr_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# create dataloader\n",
    "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 64\n",
    "trainloader_hcdr = torch.utils.data.DataLoader(hcdr_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_hcdr = torch.utils.data.DataLoader(hcdr_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_hcdr_model(\n",
    "    D_hidden=50,  \n",
    "    hidden_layers_count=1,\n",
    "    opt=optim.SGD,\n",
    "    epochs=5\n",
    "):\n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 2  # Output layer neurons - depend on what you're trying to predict, here, 2 classes: 0 and 1\n",
    "    \n",
    "    arch_string = f\"{D_in}-\"\n",
    "    arch_string += f\"{D_hidden}-\" * hidden_layers_count + f\"{D_out}\"\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, D_hidden),  # X.matmul(W1)\n",
    "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for _ in range(hidden_layers_count):\n",
    "        layers.append(torch.nn.Linear(D_hidden, D_hidden))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(D_hidden, D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # use Cross Entropy and SGD optimizer.\n",
    "    loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
    "    optimizer = opt(model.parameters(), lr=0.01)\n",
    "\n",
    "    #summary(model, (4, 20))\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    print('-'*50)\n",
    "\n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "    train_losses = {}\n",
    "    train_accuracy = {}\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = []\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        for batch, data in enumerate(trainloader_hcdr):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # do forward pass\n",
    "            output = model(inputs.float())\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(output, target)\n",
    "            # get gradients w.r.t to parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform gradient update\n",
    "            optimizer.step()\n",
    "\n",
    "            y_pred.extend(torch.argmax(output, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        epoch_training_loss = np.mean(running_loss)\n",
    "        train_losses[epoch+1] = epoch_training_loss\n",
    "        print(f\"Epoch {epoch+1}, Training loss: {np.round(epoch_training_loss, 3)}\")\n",
    "\n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum()/ correct.size\n",
    "        train_accuracy[epoch+1] = accuracy\n",
    "        print(f\"Epoch {epoch+1}, Training accuracy: {np.round(accuracy, 3)}\")\n",
    "\n",
    "    print(train_losses)\n",
    "\n",
    "    print('Finished Training')\n",
    "    print('-'*50)\n",
    "\n",
    "    test_batch_losses = []\n",
    "    test_y_pred = []\n",
    "    test_target = []\n",
    "    for batch, data in enumerate(testloader_hcdr):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)\n",
    "        # do forward pass\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        test_batch_losses.append(loss.item())\n",
    "\n",
    "        test_y_pred.extend(torch.argmax(output, dim=1).tolist())\n",
    "        test_target.extend(target.tolist())\n",
    "\n",
    "    print(f\"Test loss: {np.round(np.mean(test_batch_losses), 3)}\")\n",
    "\n",
    "    #accuracy\n",
    "    test_correct = (np.array(test_y_pred) == np.array(test_target))\n",
    "    test_accuracy = test_correct.sum()/ test_correct.size\n",
    "    test_accuracy = np.round(test_accuracy, 3)\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    \n",
    "    return arch_string, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325a162-0eeb-4b07-b016-98d1b152c042",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "**The following cell might take relatively longer to run owing to the larger size of the dataset and the parameters you choose for the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfa10c27-8efd-4356-8357-cfcf2334bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=245, out_features=30, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Epoch 1, Training loss: 0.283\n",
      "Epoch 1, Training accuracy: 0.912\n",
      "Epoch 2, Training loss: 0.254\n",
      "Epoch 2, Training accuracy: 0.919\n",
      "Epoch 3, Training loss: 0.252\n",
      "Epoch 3, Training accuracy: 0.919\n",
      "Epoch 4, Training loss: 0.252\n",
      "Epoch 4, Training accuracy: 0.919\n",
      "Epoch 5, Training loss: 0.251\n",
      "Epoch 5, Training accuracy: 0.919\n",
      "Epoch 6, Training loss: 0.251\n",
      "Epoch 6, Training accuracy: 0.919\n",
      "Epoch 7, Training loss: 0.251\n",
      "Epoch 7, Training accuracy: 0.919\n",
      "Epoch 8, Training loss: 0.25\n",
      "Epoch 8, Training accuracy: 0.919\n",
      "Epoch 9, Training loss: 0.25\n",
      "Epoch 9, Training accuracy: 0.919\n",
      "Epoch 10, Training loss: 0.25\n",
      "Epoch 10, Training accuracy: 0.919\n",
      "{1: 0.28323799115616033, 2: 0.2539224584740132, 3: 0.25214954034138404, 4: 0.2515087149675223, 5: 0.2512056253820565, 6: 0.25090992431448234, 7: 0.2506796679377715, 8: 0.2504349016832338, 9: 0.25016431774689957, 10: 0.2500485864916764}\n",
      "Finished Training\n",
      "--------------------------------------------------\n",
      "Test loss: 0.25\n",
      "Test accuracy: 0.92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245-10-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245-10-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245-10-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>20</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245-20-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245-30-30-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>91.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>245-5-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>245-40-40-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>245-40-40-2</td>\n",
       "      <td>&lt;class 'torch.optim.adam.Adam'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>245-20-20-2</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>245-30-2</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                        Optimizer Epochs Test accuracy\n",
       "0            245-10-2  <class 'torch.optim.adam.Adam'>     20         92.0%\n",
       "1            245-10-2  <class 'torch.optim.adam.Adam'>     20         92.0%\n",
       "2            245-10-2  <class 'torch.optim.adam.Adam'>     20         92.0%\n",
       "3            245-20-2  <class 'torch.optim.adam.Adam'>     10         92.0%\n",
       "4         245-30-30-2  <class 'torch.optim.adam.Adam'>     10         91.9%\n",
       "5             245-5-2  <class 'torch.optim.adam.Adam'>     10         92.0%\n",
       "6         245-40-40-2  <class 'torch.optim.adam.Adam'>     10         92.0%\n",
       "7         245-40-40-2  <class 'torch.optim.adam.Adam'>     10         92.0%\n",
       "8         245-20-20-2    <class 'torch.optim.sgd.SGD'>     10         92.0%\n",
       "9            245-30-2    <class 'torch.optim.sgd.SGD'>     10         92.0%"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve larger train/test accuracy\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(D_hidden) - The number of neurons in each hidden layer, DEFAULT: 20\n",
    "(hidden_layers_count) - The number of hidden layers in the network,  DEFAULT: 1\n",
    "(opt) - The opt function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "'''\n",
    "\n",
    "D_hidden = 30\n",
    "hidden_layers_count = 1\n",
    "opt = optim.SGD  # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 10\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, test_accuracy = run_hcdr_model(\n",
    "    D_hidden,\n",
    "    hidden_layers_count,\n",
    "    opt,\n",
    "    epochs\n",
    ")\n",
    "    \n",
    "\n",
    "try: hcdrLog \n",
    "except : hcdrLog = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Test accuracy\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "hcdrLog.loc[len(hcdrLog)] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{test_accuracy * 100}%\",\n",
    "]\n",
    "\n",
    "hcdrLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5bf93-a1ed-4722-a5e0-78b4d99e47b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
